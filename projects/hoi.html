<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Learning Explicit Contact for Implicit Reconstruction of Hand-held Objects from Monocular Images</title>
<!-- Bootstrap -->
<link href="./css/bootstrap-4.0.0.css" rel="stylesheet">
</head>
<body>
<div id="page_container">
<header>
  <div class="jumbotron">
    <div class="container">
      <div class="row">
        <div class="col-12">
          <!-- <h5 class="text-center">ECCV 2022</h5> -->
          <h2 class="text-center">Learning Explicit Contact for Implicit Reconstruction of Hand-held Objects from Monocular Images</h2>
          <!-- <p class="text-center">&nbsp;</p> -->
          <h6 class="text-center"></h6><p class="text-center"><font size="5">AAAI 2024</font></p></h6>
          <h6 class="text-center"><a href="https://junxinghu.github.io/">Junxing Hu<sup>1,2</sup></a>, <a href="https://hongwenzhang.github.io/">Hongwen Zhang<sup>3</sup></a>, <a href="https://zerchen.github.io/">Zerui Chen<sup>4</sup></a>, <a href="https://github.com/Dw1010/">Mengcheng Li<sup>5</sup></a>, <a href="https://wylcasia.github.io/">Yunlong Wang<sup>2</sup></a>, <a href="http://www.liuyebin.com/">Yebin Liu<sup>5</sup></a>, <a href="http://www.cbsr.ia.ac.cn/users/znsun/">Zhenan Sun<sup>1,2</sup></a></h6>
          <p class="text-center"><sup>1</sup>University of Chinese Academy of Sciences, <sup>2</sup>CASIA, <sup>3</sup>Beijing Normal University, <sup>4</sup>Inria, <sup>5</sup>Tsinghua University</p>
        </div>
      </div>
    </div>
  </div>
</header>
<section>
  <div class="container">
    <p>&nbsp;</p>
    <div class="row">
      <div class="col-lg-12 col-md-12 col-sm-12 col-xl-12 text-center">
        <h2>Abstract</h2>
      </div>
    </div>
  </div>
  <div class="container ">
    <div class="row">
      <div class="col-lg-12 col-md-12 col-sm-12 text-center  offset-xl-0 col-xl-12">
        <p class="text-left"><em>Reconstructing hand-held objects from monocular RGB images is an appealing yet challenging task. In this task, contacts between hands and objects provide important cues for recovering the 3D geometry of the hand-held objects. Though recent works have employed implicit functions to achieve impressive progress, they ignore formulating contacts in their frameworks, which results in producing less realistic object meshes. In this work, we explore how to model contacts in an explicit way to benefit the implicit reconstruction of hand-held objects. The proposed method <b>CHOI</b> consists of two components: <b>explicit contact prediction</b> and <b>implicit shape reconstruction</b>. In the first part, we propose a new subtask of directly estimating 3D hand-object contacts from a single image. The part-level and vertex-level graph-based transformers are cascaded and jointly learned in a coarse-to-fine manner for more accurate contact probabilities. In the second part, we introduce a novel method to diffuse estimated contact states from the hand mesh surface to nearby 3D space and leverage diffused contact probabilities to construct the implicit neural representation for the manipulated object. Benefiting from estimating the interaction patterns between the hand and the object, CHOI can reconstruct more realistic object meshes, especially for object parts that are in contact with hands. Extensive experiments on challenging benchmarks show that the proposed method outperforms the current state of the arts by a great margin.  </em></p>
        <p class="text-left">&nbsp;</p>
		    <h5 class="text-center">
          <a href="https://ojs.aaai.org/index.php/AAAI/article/view/27995">[Paper]</a>
          <a href="https://github.com/JunxingHu/CHOI">[Code]</a>
        </h5>
        <p class="text-left">&nbsp;</p>
		<img src="./assets/main_pipeline.jpg" width="1080" alt="">
        <p class="text-left">Fig 1.&nbsp;<b>The overview of learning explicit contact for implicit reconstruction.</b> In the first stage, the method estimates contact regions of the hand given a monocular RGB image. Based on the template MANO mesh, part- and vertex-level graph-based transformers are constructed and cascaded together for accurate predictions. In the second stage, the estimated contact is used to construct the implicit neural representation. An off-the-shelf module is first utilized to produce the camera parameters, hand mesh, and initial features from the image. Then, the structured contact codes are generated by anchoring contact probabilities to the hand mesh surface.  After sparse convolutions, the contact states on the hand surface are diffused to its nearby 3D space, which facilitates the perception and reconstruction of the manipulated object.</p>
        <p>&nbsp;</p>
      </div>
    </div>
    <hr>
    <div class="row">
      <div class="col-lg-12 col-md-12 col-sm-12 col-xl-12 text-center">
        <h2>Results </h2>
        <p>&nbsp;</p>
      </div>
    </div>
    <div class="row">
      <div class="col-lg-12 col-md-12 col-sm-12 text-center col-xl-12"> 
		<p>&nbsp;</p>
		<img src="./assets/contactmore.jpg" width="1000" alt="">
        <p class="text-left">Fig 2.&nbsp;<b>Contact prediction on HO3D (Rows 1-5) and OakInk (Rows 6-10) datasets.</b> The ground truth area is green, the single part or vertex-level prediction is red, and the multi-level estimation corresponds to blue. For the sample whose contact regions are occluded by hands, the hand mesh is rotated 90 or 180 degrees for clear visualization. Compared with the single-level structure, the proposed multi-level method is robust to complex hand poses and occlusions from hands or objects.
        </p>
        <p>&nbsp;</p>
    <img src="./assets/morecompre.jpg" width="1000" alt="">
        <p class="text-left">Fig 3.&nbsp;<b>Qualitative comparison with the state-of-the-art method on object reconstruction on OakInk (Rows 1-5) and HO3D (Rows 6-10) datasets.</b> Our method can reconstruct more realistic objects, especially for parts that are in contact with hands.</p>
        <p>&nbsp;</p>
      </div>
    </div>
    <div class="row">
      <div class="col-lg-12 col-md-12 col-sm-12 text-center col-xl-12">
        <p>&nbsp;</p>
		<video width="1000" height=""  muted autoplay="autoplay" loop="loop">
          <source src="assets/oakink_select_canname.mp4" type="video/mp4">
        </video>
		<video width="1000" height=""  muted autoplay="autoplay" loop="loop">
          <source src="assets/oakink_select_cameraname.mp4" type="video/mp4">
        </video>
        <p class="text-left">Fig 4.&nbsp;<b>Video results.</b> The objects are unseen during training. The results are reconstructed frame-by-frame without post-processing.
        </p>
        <p>&nbsp;</p>
        <p>&nbsp;</p>
      </div>
    </div>
    <hr>
    <div class="row">
      <div class="col-lg-12 col-md-12 col-sm-12 col-xl-12 text-center">
        <h2>Technical Paper</h2>
      </div>
    </div>
    <div class="col-lg-12 col-md-12 col-sm-12 col-xl-12 text-center"> <a href="assets/choi_hjx_aaai2024.pdf"><img src="./assets/arxiv_paper_img.png" width="1000" alt=""></a>
      <p>&nbsp;</p>
    </div>
	<!-- <hr>
    <div class="row">
      <div class="col-lg-12 col-md-12 col-sm-12 col-xl-12 text-center">
        <h2>Supplementary Material</h2>
      </div>
    </div>
    <div class="col-lg-12 col-md-12 col-sm-12 col-xl-12 text-center"> <a href="assets/supp.pdf"><img src="./assets/supp_image.jpg" width="500" alt=""></a>
      <p>&nbsp;</p>
    </div> -->
    <hr>

    <div class="row">
      <div class="col-lg-12 col-md-12 col-sm-12 col-xl-12 text-center">
        <h2>BibTeX</h2>
      </div>
    </div>
    <div class="col-lg-12 col-md-12 col-sm-12 col-xl-12 text-left">
      <pre><tt>
      @InProceedings{hu2024learning,
          title  = {Learning Explicit Contact for Implicit Reconstruction of Hand-held Objects from Monocular Images},
          author = {Hu, Junxing and Zhang, Hongwen and Chen, Zerui and Li, Mengcheng and Wang, Yunlong and Liu, Yebin and Sun, Zhenan},
          booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
          year = {2024},
      }</tt></pre>
      <p>&nbsp;</p>
      <p>&nbsp;</p>
      <p>&nbsp;</p>
    </div>
    <div class="row"> </div>
  </div>
  <div class="jumbotron"> </div>
</section>	
</div>

<!-- jQuery (necessary for Bootstrap's JavaScript plugins) --> 
<script src="./js/jquery-3.2.1.min.js"></script> 
<!-- Include all compiled plugins (below), or include individual files as needed --> 
<script src="./js/popper.min.js"></script> 
<script src="./js/bootstrap-4.0.0.js"></script>

</body><div style="all: initial;"><div id="__hcfy__" style="all: initial;"></div></div></html>